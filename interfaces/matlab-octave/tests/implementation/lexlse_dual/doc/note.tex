%
% Copyright 2013-2021 INRIA
%

\documentclass[12pt]{article}
\usepackage{fullpage,amsmath,amsfonts,verbatim,balance}
\usepackage[small,bf]{caption}

\usepackage{mathtools}
\usepackage{subfigure}

\usepackage{graphicx,color,psfrag}
\usepackage{epsfig}
\usepackage{hyperref}
\usepackage{color}

\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}

\usepackage{bbm}

\input defs.tex

% \todo I don't like the notation x^{(k)}. To use something different. Maybe I should use
% $\tilde{x}_k^{\star}$ instead of $x_{k}^{\star}$ (as we do i the paper).

\title{Tikhonov regularization and Lagrange multipliers} \author{Dimitar Dimitrov}

%\date{} % to remove the date

\begin{document}
\maketitle

\section{Tikhonov regularization}

\subsection{First objective}

We want to
%
\begin{align*}
\minimize{x} \norm{A_1x - y_1}^2 + \mu_1^2\norm{x}^2,
\end{align*}
%
or equivalently
%
\begin{align*}
\minimize {x} \norm{\begin{bmatrix} A_1 \\ \mu_1I \end{bmatrix}x - \begin{bmatrix} y_1 \\ 0 \end{bmatrix}}^2.
\end{align*}
%
Factorizing $A_1 = Q_1^{'}D_1$, with $D_1 = \begin{bmatrix} R_1 & T_1\end{bmatrix}$ (no permutations
  for simplicity) leads to
%
\begin{align} \label{eq.LS1}
  \minimize {x} \norm{\begin{bmatrix} D_1 \\ \mu_1I \end{bmatrix}x -
    \begin{bmatrix} x_1^{\star} \\ 0 \end{bmatrix}}^2,
\end{align}
%
where $x_1^{\star} = Q_1^{'T}y_1 \in \reals^{p_1}$. The solution of~\eqref{eq.LS1}, $x^{(1)}$, can
be found using the Normal equations
%
\begin{align*}
  x^{(1)} = (D_1^TD_1 + \mu_1^2I)^{-1}D_1^Tx_1^{\star}.
\end{align*}
%
This solution leads to the residual $r_1^{\star} = A_1x^{(1)} - y_1$, which should the achieved
using our basic solution $Y_1x_1^{\star}$ (for this level). That is
%
\[
\underbrace{Q_1^{'}\begin{bmatrix} R_1 & T_1\end{bmatrix}}_{A_1}
  \underbrace{\begin{bmatrix} R_1^{-1} \\ 0\end{bmatrix}}_{Y_1}x_1^{\star} - y_1 = r_1^{\star}.
\]
%
Hence, we should modify $x_1^{\star} = Q_1^{'T}y_1$ to $ x_1^{\star} = Q_1^{'T}(y_1 + r_1^{\star}) =
D_1x^{(1)}$ (before we proceed with the elimination step).
%

\newpage

\subsection{Second objective}

Using the basis
%
\begin{align*}
B_1 = \begin{bmatrix} Y_1 & Z_1 \end{bmatrix} =
\begin{bmatrix} R_1^{-1} & -R_1^{-1}T_1 \\ 0 & I \end{bmatrix},
\end{align*}
%
we want to
%
\begin{align*}
  \minimize{\bar{x}_1} \norm{A_2B_1\begin{bmatrix} x_1^{\star} \\ \bar{x}_1 \end{bmatrix} - y_2}^2 +
  \mu_2^2\norm{B_1\begin{bmatrix} x_1^{\star} \\ \bar{x}_1 \end{bmatrix}}^2.
\end{align*}
%
Note that
%
\[
x = B_1\begin{bmatrix} x_1^{\star} \\ \bar{x}_1 \end{bmatrix}.
\]
%
Equivalently, we have
%
\begin{align*}
  \minimize{\bar{x}_1} \norm{\begin{bmatrix}A_2Z_1 \\ \mu_2Z_1\end{bmatrix}\bar{x}_1 -
      \begin{bmatrix} (y_2 - A_2Y_1x_1^{\star}) \\ -\mu_2Y_1x_1^{\star}\end{bmatrix} }^2.
\end{align*}
%
Factorizing $A_2Z_1 = Q_2^{'}D_2$, with $D_2 = \begin{bmatrix} R_2 & T_2\end{bmatrix}$ leads to
%
\begin{align} \label{eq.LS2}
  \minimize {\bar{x}_1} \norm{\begin{bmatrix} D_2 \\ \mu_2Z_1 \end{bmatrix}\bar{x}_1 -
    \begin{bmatrix} x_2^{\star} \\ -\mu_2Y_1x_1^{\star} \end{bmatrix}}^2,
\end{align}
%
where $x_2^{\star} = y_2 - A_2Y_1x_1^{\star} \in \reals^{p_2}$. Note that $A_2Y_1 = L_{21}$ (from the $\ell$-QR
factorization), and $x_2^{\star}$ is directly obtained during the elimination step. Let us
rewrite~\eqref{eq.LS2} as
%
\begin{align} \label{eq.LS2_other}
  \minimize {\bar{x}_1} \norm{\begin{bmatrix} D_2 \\ S_1 \\ \mu_2I \end{bmatrix}\bar{x}_1 -
    \begin{bmatrix} x_2^{\star} \\ s_1 \\ 0 \end{bmatrix}}^2,
\end{align}
%
where $s_1 = -\mu_2R_1^{-1}x_1^{\star}$ and $S_1 = -\mu_2R_1^{-1}T_1$. The solution
of~\eqref{eq.LS2_other}, $\bar{x}_1^{\star}$, can be found using the Normal equations.
%
\begin{align} \label{eq.Normal}
  \bar{x}_1^{\star} = (D_2^TD_2 + S_1^TS_1 + \mu_2^2I)^{-1} (D_2^Tx_2^{\star} + S_1^Ts_1).
\end{align}
%
Given $\bar{x}_1^{\star}$, we have to modify $x_2^{\star} = D_2\bar{x}_1^{\star}$.

Note that only the lower triangular part of the matrix in~\eqref{eq.Normal} is needed in order to
find its Cholesky decomposition (see {\color{blue}\verb|regularize_tikhonov_1|}). The function
{\color{blue}\verb|regularize_tikhonov_2|} uses a different approach ({\color{red}to describe}). For
the computation of the Lagrange multipliers, we would also need to compute $x^{(2)}$. This requires
a backward substitution.

\newpage

\subsection{$k$-th objective}

For the $k$-th objective we have to solve
%
\begin{align*}
  \minimize {\bar{x}_{k-1}} \norm{\begin{bmatrix} D_k \\ S_{k-1} \\ \mu_kI \end{bmatrix}\bar{x}_{k-1} -
    \begin{bmatrix} x_{k}^{\star} \\ s_{k-1} \\ 0 \end{bmatrix}}^2,
\end{align*}
%
with $D_k = \begin{bmatrix} R_k & T_k\end{bmatrix} = Q_k^{'T}A_kZ_1,\dots,Z_{k-1}$. As $k$
  increases, the column dimension of $S_{k}$ shrinks but its row dimension increases. In the code,
  the matrices $S_k$ can be formed using the function
  {\color{blue}\verb|accumulate_nullspace_basis|} or
  {\color{blue}\verb|accumulate_nullspace_basis_1|}. As an example, consider the product
  %
  \begin{align*}
    \underbrace{\begin{bmatrix} A & B \\ \multicolumn{2}{c}{I} \end{bmatrix}}_{Z_1}
    \underbrace{\begin{bmatrix} C \\ I \end{bmatrix}}_{Z_2} =
    \underbrace{\begin{bmatrix} AC + B \\ C \\ I \end{bmatrix}}_{Z_1Z_2} =
    \begin{bmatrix} S_2 \\ I \end{bmatrix},
  \end{align*}
  %
  where $S_1 = -R_1^{-1}T_1$ is split in $\begin{bmatrix} A & B \end{bmatrix}$ such that the number of
  columns of $A$ is the same as the number of rows of $C$.

  \newpage

\section{Problem statement}

Consider the following sequence of problems
%
\begin{align*}
  \begin{split}
    \minimize{x_k, \, v_k} & \frac{1}{2}\norm{v_k}^2 + \frac{\mu_k^2}{2}\norm{x_k}^2\\
    \st & b \leq Cx_k - v \leq u \\
    & v_j = v_j^{\star}, \quad j=1,\dots,k-1.
  \end{split}
\end{align*}
%
for $k=1,\dots,P$. The pair $(v_k^{\star},x_k^{\star})$ is the unique solution to the $k$-th
problem. We want to find $(v_P^{\star},x_P^{\star})$ by repeatedly solving
%
\begin{align}
  \begin{split} \label{eq.lexlse}
    \minimize{x_k, \, r_k} & \frac{1}{2}\norm{r_k}^2 + \frac{\mu_k^2}{2}\norm{x_k}^2\\
    \st & r = Ax_k - y \\
    & r_j = r_j^{\star}, \quad j=1,\dots,k-1,
  \end{split}
\end{align}
%
for a given guess of the active constraints defined in terms of $(A,y)$.

\section{A note related to our current approach}

For the $k$-th problem in~\eqref{eq.lexlse} we have to compute $x_k^{\star}$ and use
%
\[
A^T\lambda_{k}^{\star} = -\mu_k^2x^{\star}_k
\]
%
for computing $\lambda_{k}^{\star}$. Using the compact $\ell$-QR decomposition, we have
%
\[
\mathnormal{\Pi}\begin{bmatrix}R_{\ell}^T \\ T_{\ell}^T\end{bmatrix}
Q_{\ell}^{'T}\lambda_{k}^{\star} = -\mu_k^2x^{\star}_k.
\]
%
Note that $R_{\ell}^T$ is full column rank, while $Q_{\ell}^{'T}$ is full row rank, and
$\mathnormal{\Pi}$ is a permutation matrix. Let $z_k = Q_{\ell}^{'T}\lambda_{k}^{\star}$ and $d_k =
-\mu^2\mathnormal{\Pi}^Tx^{\star}_k$. Thus, we have to solve
%
\[
\underbrace{\begin{bmatrix}R_{\ell}^T \\ T_{\ell}^T\end{bmatrix}}_{D_{\ell}}z_k = \underbrace{\begin{bmatrix} d_k^{'} \\ d_k^{''}\end{bmatrix}}_{d_k}.
\]
%
Note, however, that $d_k \in \mathcal{R}(D_{\ell})$, hence $z = R_{\ell}^{-T}d_k^{'}$ (which can be
obtained using a backward substitution). Then we have to solve $Q_{\ell}^{'T}\lambda_{k}^{\star} =
z$ to find $\lambda_{k}^{\star}$ (here we can reuse our current implementation for solving
$Q_{\ell}^{'T}\lambda_{k}^{\star} = 0$).

\section{A dual approach}

For $k=1,\dots,P$ compute a pair $(x_k^{\star},\lambda_{k}^{\star})$ using
%
\begin{align*}
\lambda_k^{\star} \in \argmin{\lambda_{k}} & \norm{B_k\lambda_{k} - b_k}^2, \quad
x_k^{\star} = -\frac{1}{\mu_k^2}\sum_{i=1}^{k}A_i^T\lambda_{ki}^{\star},
\end{align*}
%
where
%
\begin{align*}
B_k = \begin{bmatrix}A_1^T & \dots & A_k^T \\ 0 & \dots & \mu_kI \end{bmatrix}, \quad
b_k = -\mu_k\begin{bmatrix} \mu_k x_{k-1}^{\star} \\ y_k - A_kx_{k-1}^{\star} \end{bmatrix}, \quad
\lambda_{k} = \begin{bmatrix} \lambda_{k1} \\ \vdots \\ \lambda_{kk} \end{bmatrix} \in \mathbb{R}^{d_k}, \quad
d_k = \sum_{i=1}^{k}m_i.
\end{align*}
%
The recursion in initialized with $x_0^{\star} = 0$. After performing the $P$ steps from above, one
obtains a solution pair $(x^{\star}, \Lambda^{\star})$ with
%
\begin{align*}
x^{\star} = x_P^{\star}, \quad
\Lambda^{\star} =
\begin{bmatrix}
  \lambda_{11}^{\star} & \lambda_{21}^{\star} & \dots  &\lambda_{P1}^{\star} \\
                       & \lambda_{22}^{\star} & \dots  &\lambda_{P2}^{\star} \\
                       &                      & \ddots & \vdots \\
                       &                      &        &\lambda_{PP}^{\star} \\
\end{bmatrix}.
\end{align*}

\newpage

\section{Derivation (of the dual approach)}

\subsection{First objective}

\begin{itemize}

\item The primal problem
  %
  \begin{align*}
    \begin{split}
      \minimize{x, \, r_1} & f_1(r_1) = \frac{1}{2}\norm{r_1}^2 + \frac{\mu_1^2}{2}\norm{x}^2\\
      \st & r_1 = A_1x - y_1.
    \end{split}
  \end{align*}

\item The Lagrangian
  %
  \[
  L_1(x,r_1,\lambda_{11}) = \frac{1}{2}r_1^Tr_1 + \frac{\mu_1^2}{2} x^Tx + \lambda_{11}^T(A_1x - y_1 - r_1).
  \]

\item The dual function
%
\[
g_1(\lambda_{11}) = \inf_{x,r_1} \underbrace{\frac{1}{2}r_1^Tr_1 - \lambda_{11}^Tr_1} +
\underbrace{\frac{\mu_1^2}{2}x^Tx + \lambda_{11}^TA_1x} - \lambda_{11}^Ty_1.
\]

Minimizing w.r.t. $x$ and then $r_1$ gives
%
\begin{align} \label{eq.x_r}
x^{\star} = -\frac{1}{\mu_1^2}A_1^T\lambda_{11}, \quad r_1^{\star} = \lambda_{11}.
\end{align}
%
Substituting back leads to
%
\begin{align*}
\frac{1}{2}r_1^Tr_1 - \lambda_{11}^Tr_1 &= -\frac{1}{2}\lambda_{11}^T\lambda_{11} \\
\frac{\mu_1^2}{2} x^Tx + \lambda_{11}^TA_1x &= -\frac{1}{2\mu_1^2} \lambda_{11}^T A_1A_1^T \lambda_{11}.
\end{align*}
%
Hence,
%
\begin{align*}
g_1(\lambda_{11}) = -\frac{1}{2\mu_1^2}\lambda_{11}^T\left(\mu_1^2I+A_1A_1^T\right)\lambda_{11} - \lambda_{11}^Ty_1.
\end{align*}
%
Note that due to the regularization, the dual function cannot take infinite values and hence there
is no need to have constraints.

\item The dual problem
%
\begin{align*}
  \minimize{\lambda_{11}} -g_1(\lambda_{11}) = \frac{1}{2\mu_1^2}\lambda_{11}^T\left(\mu_1^2 I+A_1A_1^T\right)\lambda_{11} + \lambda_{11}^Ty_1,
\end{align*}
%
or equivalently
%
\begin{align*}
  \minimize{\lambda_{11}} \lambda_{11}^T\left(\mu_1^2 I+A_1A_1^T\right)\lambda_{11} + 2\mu_1^2\lambda_{11}^Ty_1.
\end{align*}
%
This can be formulated as the following least-squares problem
%
\begin{align*}
\minimize{\lambda_{11}} & \norm{\begin{bmatrix}A_1^T \\ \mu_1I \end{bmatrix}\lambda_{11} - \begin{bmatrix} 0 \\ \mu_1y_1 \end{bmatrix}}^2.
\end{align*}

\end{itemize}

\newpage

\subsection{Second objective}

\begin{itemize}

\item The primal problem
  %
  \begin{align*}
    \begin{split}
      \minimize{x, \, r_2} & f_2(r_2) = \frac{1}{2}\norm{r_2}^2 + \frac{\mu_2^2}{2}\norm{x}^2\\
      \st & r_2 = A_2x - y_2 \\
      & r_1^{\star} = A_1x - y_1.
    \end{split}
  \end{align*}

\item The Lagrangian
  %
  \[
  L_1(x,r_2,\lambda_{21},\lambda_{22}) = \frac{1}{2}r_2^Tr_2 + \frac{\mu_2^2}{2} x^Tx + \lambda_{21}^T(A_1x - y_1 - r_1^{\star}) + \lambda_{22}^T(A_2x - y_2 - r_2).
  \]

\item The dual function
%
\[
g_1(\lambda_{21}, \lambda_{22}) = \inf_{x,r_2} \underbrace{\frac{1}{2}r_2^Tr_2 - \lambda_{22}^Tr_2} + \underbrace{\frac{\mu_2^2}{2}x^Tx + \lambda_{21}^TA_1x + \lambda_{22}^TA_2x} -
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T \begin{bmatrix} y_1+r_1^{\star} \\ y_2 \end{bmatrix}.
\]

Minimizing w.r.t. $x$ and then $r_2$ gives
%
\begin{align} \label{eq.x_r}
x^{\star} = -\frac{1}{\mu_2^2}\sum_{i=1}^{2}A_i^T\lambda_{2i}, \quad r_2^{\star} = \lambda_{22}.
\end{align}
%
Substituting back leads to
%
\begin{align*}
\frac{1}{2}r_2^Tr_2 - \lambda_{22}^Tr_2 &= -\frac{1}{2}\lambda_{22}^T\lambda_{22} \\
\frac{\mu_2^2}{2} x^Tx + \lambda_{21}^TA_1x + \lambda_{22}^TA_2x
&= -\frac{1}{2\mu_2^2} \begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T
\begin{bmatrix} A_1A_1^T & A_1A_2^T \\ A_2A_1^T & A_2A_2^T \end{bmatrix}
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}.
\end{align*}
%
Hence,
%
\begin{align*}
g_2(\lambda_{21},\lambda_{22}) =
-\frac{1}{2\mu_2^2} \begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T
\begin{bmatrix} A_1A_1^T & A_1A_2^T \\ A_2A_1^T & \mu_2^2 I+A_2A_2^T \end{bmatrix}
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix} -
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T \begin{bmatrix} y_1+r_1^{\star} \\ y_2 \end{bmatrix}.
\end{align*}

\item The dual problem
%
\begin{align*}
\minimize{\lambda_{2}} &
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T
\begin{bmatrix} A_1A_1^T & A_1A_2^T \\ A_2A_1^T & \mu_2^2 I+A_2A_2^T \end{bmatrix}
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix} +
2\mu_2^2
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T \begin{bmatrix} y_1+r_1^{\star} \\ y_2 \end{bmatrix}.
\end{align*}
%
This can be formulated as the following least-squares problem
%
\begin{align*}
  \minimize{\lambda_{21},\ \lambda_{22}} & \norm{\begin{bmatrix}A_1^T & A_2^T \\ 0 & \mu_2I \end{bmatrix}
    \begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix} -
    \mu_2\begin{bmatrix} -\mu_2 x_1^{\star} \\ -\left(y_2 - A_2x_1^{\star}\right) \end{bmatrix}}^2.
\end{align*}

\end{itemize}

\section{Cycling example} \label{seq.Cycling}

Consider a lexicographic problem with the following objectives
%
\begin{itemize}
  \item objective 1
    %
    \[
    y \leq -1
    \]

  \item objective 2
    %
    \begin{align*}
    x &= 1 \nonumber \\
    x + \alpha y &= 0 \nonumber
    \end{align*}
\end{itemize}
%
For convenience, define
%
\[
A_1 = \begin{bmatrix} 0 & 1 \end{bmatrix}, \quad
A_2 = \begin{bmatrix} 1 & 0 \\ 1 & \alpha \end{bmatrix}, \quad
b_2 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad
v = \begin{bmatrix} x \\ y \end{bmatrix}.
\]

\subsection{$y \leq -1$ is active}

The solution is given by
%
\begin{align} \label{eq.solution}
v = \begin{bmatrix} \frac{1}{2}(1+\alpha) \\ -1 \end{bmatrix}.
\end{align}
%
%
The residual for the second task is given by
%
\[
w_2 = A_2 v - b_2 =
\begin{bmatrix} \beta \\ -\beta \end{bmatrix}, \quad \beta = \frac{1}{2}(\alpha-1).
\]
%
For the Lagrange multipliers we have
\[
A_1^T\lambda_1 + A_2^Tw_2 =
\begin{bmatrix} 0 \\ 1 \end{bmatrix}\lambda_1 +
\begin{bmatrix} 0 \\ -\alpha\beta \end{bmatrix} =
\begin{bmatrix} 0 \\ 0 \end{bmatrix}.
\]
%
Hence, $\lambda_1 = \alpha\beta = \frac{\alpha}{2}(\alpha-1)$. $\lambda_1$ is negative for $0 <
\alpha < 1$, \emph{e.g.}, for $\alpha = 0.1$, $\lambda_1 = -0.045$. Obviously, dropping the
inequality constraint is the better choice when the constraints of the second objective are
considered to be linearly independent (if not, we have cycling).

\subsection{$y \leq -1$ is inactive}

\[
Q = c\begin{bmatrix} 1 & -1 \\ 1 & 1\end{bmatrix}, \quad
R = c\begin{bmatrix} 2 & \alpha \\ 0 & \alpha \end{bmatrix}, \quad
A_2 = QR,
\]
%
with $c = \frac{\sqrt{2}}{2}$. If $c\alpha$ is considered small, then the residual does not depend
on $\alpha$. Note that the Lagrange multipliers above depend on $\alpha$. This discrepancy is what
causes the cycling in this case.

\subsection{Notes}

If $c\alpha$ is considered to be small, we would have
%
\[
c^2\begin{bmatrix} 1 \\ 1 \end{bmatrix}
\begin{bmatrix} 2 & \alpha \end{bmatrix} =
\frac{1}{2}\begin{bmatrix} 2 & \alpha \\ 2 & \alpha \end{bmatrix}.
\]
%
Hence in the case $y \leq -1$ we would actually be solving
%
\[
\underbrace{\frac{1}{2}\begin{bmatrix} 2 & \alpha \\ 2 & \alpha \end{bmatrix}}_{A_2^{'}}
\begin{bmatrix} x \\y \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}.
\]

Now, consider the hierarchy
%
\begin{itemize}
\item $A_1v = -1$
\item $A_2^{'}v = b_2$
\end{itemize}
%
The solution is the same as in~\eqref{eq.solution}. The residual is given by
%
\[
w_2^{'} = A_2^{'} v - b_2 =
\frac{1}{2}\begin{bmatrix} -1 \\ 1 \end{bmatrix}.
\]
%
Most importantly $A_2^{'T}w_2^{'} = 0$, which leads to $\lambda_1^{'} = 0$. Hence, there would be no
cycling (for any choice of tolerances). Note as well that $A_2^{'T}w_2 = 0$. Essentially what we
have to do is to apply a correction to the Lagrange multipliers in order for them to reflect our
choice of linear dependence tolerance. One way to do this would be to first estimate the rank of the
second object (\eg, by QR factorizing it) and during the elimination step to work only with the
non-singular part.

It seems that here we would have to seriously consider the order of computations in order to
increase reliability. On this example, it appears that approaching the problem in reverse (something
similar to Flacco's ``reverse priority approach'') might lead to better results (but probably it
would be more expensive).

If we set the linear dependence tolerance to zero (or something veeery small), then we shouldn't
have such problems (at least in theory) but this doesn't strike me as a reasonable way to go. I
think that this would pose significant challenges to the trust region method.

\newpage

\section{Modify the bounds of the inequalities}

\subsection{A direct truncation approach}

Maybe it would be possible to reduce cycling (see Section~\ref{seq.Cycling}) by appropriately
modifying the lower and upper bounds of the inequalities. Consider one level of an equality
constrained problem
%
\begin{align*}
  Ax = y,
\end{align*}
%
and its factorization
%
\begin{align*}
  A = \begin{bmatrix} Q^{'}
    & Q^{''} & Q^{'''} \end{bmatrix}
  \begin{bmatrix}
    R & T_1 & T_2 \\
    0 & S   & S_2 \\
    0& 0   & 0
  \end{bmatrix} \mathnormal{\Pi}^T,
\end{align*}
%
with $S\approx0$ and $S_2\approx 0$. The (basic) solution that we compute is given by
%
\begin{align} \label{eq.basic_solution}
  x = \mathnormal{\Pi}\begin{bmatrix} R^{-1}Q^{'T}y \\ 0 \end{bmatrix},
\end{align}
%
and the residual is
%
\begin{align*}
  r = -(Q^{''}Q^{''T} + Q^{'''}Q^{'''T})y.
\end{align*}
%
If we want to modify $y$ so that the residual of $Ax-\hat{y}$ is zero, we simply have to project $y$
on the range of $A$, \ie, $\hat{y} = Q^{'}Q^{'T}y$. But we don't want that because then all
residuals would be zero. Probably we want to keep the $Q^{'''}Q^{'''T}y$ part of the residual, and
get rid of the $Q^{''}Q^{''T}y$ part. This can be achieved by using
%
\begin{align*}
\hat{y} = (Q^{'}Q^{'T} + Q^{'''}Q^{'''T})y.
\end{align*}
%
In this way $Q^{''}Q^{''T}\hat{y} = 0$ and hence
%
\begin{align} \label{eq.residual}
  \hat{r} = -Q^{'''}Q^{'''T}\hat{y}.
\end{align}
%
We need to have two tolerances $\epsilon^{'}$ and $\epsilon^{''}$, where $\epsilon^{'} \leq
\abs{\diag(R)}$, $\epsilon^{''} \leq \abs{\diag(S)} < \epsilon^{'}$.

Note that our decomposition would terminate when pivots become smaller than $\epsilon^{'}$, but we
have to obtain $\begin{bmatrix} S & S_2\end{bmatrix}$ using additional Householder transformations
  applied as well to the corresponding part of $y$. Column permutations are required, but they
  should not be mixed with the ones from our factorization. All we need to do is find where $S$ ends
  and $0$ begins so that later we could use~\eqref{eq.residual} to compute the residual (required
  for computing the Lagrange multipliers). We need to store the Householder transformations.

\subsection{Using regularization}

Some similarities with the above approach ({\color{red}to explain}).

\subsection{Comment of using very small $\epsilon^{'}$}

Using a very small $\epsilon^{'}$ is a bad idea not only because $R^{-1}$ is used
in~\eqref{eq.basic_solution}, but because it appears in the elimination steps of the $\ell$-QR
decomposition. When we have many levels, our decomposition becomes very sensitive to numerical
errors. For example the solution of the same equality-constrained problem (taken from Nestor's
examples) on a $64$-bit and $32$ bit architecture (running the same \verb|UBUNTU| and using the same
version of \verb|g++|) could differ by a factor of \verb|1e+08|. Or even on the same machine, the
solution of one equality constrained problem computed within the active-set method and the solution
of the same equality constrained problem computed manually post-factum could differ by a factor of
\verb|1e-02| (on the same example as above).


\end{document}

% ---------------------------------------------------------------------------
