\documentclass[12pt]{article}
\usepackage{fullpage,amsmath,amsfonts,verbatim,balance}
\usepackage[small,bf]{caption}

\usepackage{mathtools}
\usepackage{subfigure}

\usepackage{graphicx,color,psfrag}
\usepackage{epsfig}
\usepackage{hyperref}
\usepackage{color}

\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}

\usepackage{bbm}

\input defs.tex

% \todo I don't like the notation x^{(k)}. To use something different. Maybe I should use
% $\tilde{x}_k^{\star}$ instead of $x_{k}^{\star}$ (as we do i the paper).

\title{Tikhonov regularization and Lagrange multipliers} \author{Dimitar Dimitrov}

%\date{} % to remove the date

\begin{document}
\maketitle

\section{Tikhonov regularization}

\subsection{First objective}

We want to
%
\begin{align*}
\minimize{x} \norm{A_1x - y_1}^2 + \mu_1^2\norm{x}^2,
\end{align*}
%
or equivalently
%
\begin{align*}
\minimize {x} \norm{\begin{bmatrix} A_1 \\ \mu_1I \end{bmatrix}x - \begin{bmatrix} y_1 \\ 0 \end{bmatrix}}^2.
\end{align*}
%
Factorizing $A_1 = Q_1^{'}D_1$, with $D_1 = \begin{bmatrix} R_1 & T_1\end{bmatrix}$ (no permutations
  for simplicity) leads to
%
\begin{align} \label{eq.LS1}
  \minimize {x} \norm{\begin{bmatrix} D_1 \\ \mu_1I \end{bmatrix}x -
    \begin{bmatrix} x_1^{\star} \\ 0 \end{bmatrix}}^2,
\end{align}
%
where $x_1^{\star} = Q_1^{'T}y_1 \in \reals^{p_1}$. The solution of~\eqref{eq.LS1}, $x^{(1)}$, can
be found using the Normal equations
%
\begin{align*}
  x^{(1)} = (D_1^TD_1 + \mu_1^2I)^{-1}D_1^Tx_1^{\star}.
\end{align*}
%
This solution leads to the residual $r_1^{\star} = A_1x^{(1)} - y_1$, which should the achieved
using our basic solution $Y_1x_1^{\star}$ (for this level). That is
%
\[
\underbrace{Q_1^{'}\begin{bmatrix} R_1 & T_1\end{bmatrix}}_{A_1}
  \underbrace{\begin{bmatrix} R_1^{-1} \\ 0\end{bmatrix}}_{Y_1}x_1^{\star} - y_1 = r_1^{\star}.
\]
%
Hence, we should modify $x_1^{\star} = Q_1^{'T}y_1$ to $ x_1^{\star} = Q_1^{'T}(y_1 + r_1^{\star}) =
D_1x^{(1)}$ (before we proceed with the elimination step).
%

\newpage

\subsection{Second objective}

Using the basis
%
\begin{align*}
B_1 = \begin{bmatrix} Y_1 & Z_1 \end{bmatrix} =
\begin{bmatrix} R_1^{-1} & -R_1^{-1}T_1 \\ 0 & I \end{bmatrix},
\end{align*}
%
we want to
%
\begin{align*}
  \minimize{\bar{x}_1} \norm{A_2B_1\begin{bmatrix} x_1^{\star} \\ \bar{x}_1 \end{bmatrix} - y_2}^2 +
  \mu_2^2\norm{B_1\begin{bmatrix} x_1^{\star} \\ \bar{x}_1 \end{bmatrix}}^2.
\end{align*}
%
Note that
%
\[
x = B_1\begin{bmatrix} x_1^{\star} \\ \bar{x}_1 \end{bmatrix}.
\]
%
Equivalently, we have
%
\begin{align*}
  \minimize{\bar{x}_1} \norm{\begin{bmatrix}A_2Z_1 \\ \mu_2Z_1\end{bmatrix}\bar{x}_1 -
      \begin{bmatrix} (y_2 - A_2Y_1x_1^{\star}) \\ -\mu_2Y_1x_1^{\star}\end{bmatrix} }^2.
\end{align*}
%
Factorizing $A_2Z_1 = Q_2^{'}D_2$, with $D_2 = \begin{bmatrix} R_2 & T_2\end{bmatrix}$ leads to
%
\begin{align} \label{eq.LS2}
  \minimize {\bar{x}_1} \norm{\begin{bmatrix} D_2 \\ \mu_2Z_1 \end{bmatrix}\bar{x}_1 -
    \begin{bmatrix} x_2^{\star} \\ -\mu_2Y_1x_1^{\star} \end{bmatrix}}^2,
\end{align}
%
where $x_2^{\star} = y_2 - A_2Y_1x_1^{\star} \in \reals^{p_2}$. Note that $A_2Y_1 = L_{21}$ (from the $\ell$-QR
factorization), and $x_2^{\star}$ is directly obtained during the elimination step. Let us
rewrite~\eqref{eq.LS2} as
%
\begin{align} \label{eq.LS2_other}
  \minimize {\bar{x}_1} \norm{\begin{bmatrix} D_2 \\ S_1 \\ \mu_2I \end{bmatrix}\bar{x}_1 -
    \begin{bmatrix} x_2^{\star} \\ s_1 \\ 0 \end{bmatrix}}^2,
\end{align}
%
where $s_1 = -\mu_2R_1^{-1}x_1^{\star}$ and $S_1 = -\mu_2R_1^{-1}T_1$. The solution
of~\eqref{eq.LS2_other}, $\bar{x}_1^{\star}$, can be found using the Normal equations.
%
\begin{align} \label{eq.Normal}
  \bar{x}_1^{\star} = (D_2^TD_2 + S_1^TS_1 + \mu_2^2I)^{-1} (D_2^Tx_2^{\star} + S_1^Ts_1).
\end{align}
%
Given $\bar{x}_1^{\star}$, we have to modify $x_2^{\star} = D_2\bar{x}_1^{\star}$.

Note that only the lower triangular part of the matrix in~\eqref{eq.Normal} is needed in order to
find its Cholesky decomposition (see {\color{blue}\verb|regularize_tikhonov_1|}). The function
{\color{blue}\verb|regularize_tikhonov_2|} uses a different approach ({\color{red}to describe}). For
the computation of the Lagrange multipliers, we would also need to compute $x^{(2)}$. This requires
a backward substitution.

\newpage

\subsection{$k$-th objective}

For the $k$-th objective we have to solve
%
\begin{align*}
  \minimize {\bar{x}_{k-1}} \norm{\begin{bmatrix} D_k \\ S_{k-1} \\ \mu_kI \end{bmatrix}\bar{x}_{k-1} -
    \begin{bmatrix} x_{k}^{\star} \\ s_{k-1} \\ 0 \end{bmatrix}}^2,
\end{align*}
%
with $D_k = \begin{bmatrix} R_k & T_k\end{bmatrix} = Q_k^{'T}A_kZ_1,\dots,Z_{k-1}$. As $k$
  increases, the column dimension of $S_{k}$ shrinks but its row dimension increases. In the code,
  the matrices $S_k$ can be formed using the function
  {\color{blue}\verb|accumulate_nullspace_basis|} or
  {\color{blue}\verb|accumulate_nullspace_basis_1|}. As an example, consider the product
  %
  \begin{align*}
    \underbrace{\begin{bmatrix} A & B \\ \multicolumn{2}{c}{I} \end{bmatrix}}_{Z_1}
    \underbrace{\begin{bmatrix} C \\ I \end{bmatrix}}_{Z_2} =
    \underbrace{\begin{bmatrix} AC + B \\ C \\ I \end{bmatrix}}_{Z_1Z_2} =
    \begin{bmatrix} S_2 \\ I \end{bmatrix},
  \end{align*}
  %
  where $S_1 = -R_1^{-1}T_1$ is split in $\begin{bmatrix} A & B \end{bmatrix}$ such that the number of
  columns of $A$ is the same as the number of rows of $C$.

  \newpage

\section{Problem statement}

Consider the following sequence of problems
%
\begin{align}
  \begin{split}
    \minimize{x_k, \, v_k} & \frac{1}{2}\norm{v_k}^2 + \frac{\mu_k^2}{2}\norm{x_k}^2\\
    \st & b \leq Cx_k - v \leq u \\
    & v_j = v_j^{\star}, \quad j=1,\dots,k-1.
  \end{split}
\end{align}
%
for $k=1,\dots,P$. The pair $(v_k^{\star},x_k^{\star})$ is the unique solution to the $k$-th
problem. We want to find $(v_P^{\star},x_P^{\star})$ by repeatedly solving
%
\begin{align}
  \begin{split} \label{eq.lexlse}
    \minimize{x_k, \, r_k} & \frac{1}{2}\norm{r_k}^2 + \frac{\mu_k^2}{2}\norm{x_k}^2\\
    \st & r = Ax_k - y \\
    & r_j = r_j^{\star}, \quad j=1,\dots,k-1,
  \end{split}
\end{align}
%
for a given guess of the active constraints defined in terms of $(A,y)$.

\section{A note related to our current approach}

For the $k$-th problem in~\eqref{eq.lexlse} we have to compute $x_k^{\star}$ and use
%
\[
A^T\lambda_{k}^{\star} = -\mu_k^2x^{\star}_k
\]
%
for computing $\lambda_{k}^{\star}$. Using the compact $\ell$-QR decomposition, we have
%
\[
\mathnormal{\Pi}\begin{bmatrix}R_{\ell}^T \\ T_{\ell}^T\end{bmatrix}
Q_{\ell}^{'T}\lambda_{k}^{\star} = -\mu_k^2x^{\star}_k.
\]
%
Note that $R_{\ell}^T$ is full column rank, while $Q_{\ell}^{'T}$ is full row rank, and
$\mathnormal{\Pi}$ is a permutation matrix. Let $z_k = Q_{\ell}^{'T}\lambda_{k}^{\star}$ and $d_k =
-\mu^2\mathnormal{\Pi}^Tx^{\star}_k$. Thus, we have to solve
%
\[
\underbrace{\begin{bmatrix}R_{\ell}^T \\ T_{\ell}^T\end{bmatrix}}_{D_{\ell}}z_k = \underbrace{\begin{bmatrix} d_k^{'} \\ d_k^{''}\end{bmatrix}}_{d_k}.
\]
%
Note, however, that $y \in \mathcal{R}(D_{\ell})$, hence $z = R_{\ell}^{-T}d_k^{'}$ (which can be
obtained using a backward substitution). Then we have to solve $Q_{\ell}^{'T}\lambda_{k}^{\star} =
z$ to find $\lambda_{k}^{\star}$ (here we can reuse our current implementation for solving
$Q_{\ell}^{'T}\lambda_{k}^{\star} = 0$).

\section{A dual approach}

For $k=1,\dots,P$ compute a pair $(x_k^{\star},\lambda_{k}^{\star})$ using
%
\begin{align}
\lambda_k^{\star} \in \argmin{\lambda_{k}} & \norm{B_k\lambda_{k} - b_k}^2, \quad
x_k^{\star} = -\frac{1}{\mu_k^2}\sum_{i=1}^{k}A_i^T\lambda_{ki}^{\star},
\end{align}
%
where
%
\begin{align*}
B_k = \begin{bmatrix}A_1^T & \dots & A_k^T \\ 0 & \dots & \mu_kI \end{bmatrix}, \quad
b_k = -\mu_k\begin{bmatrix} \mu_k x_{k-1}^{\star} \\ y_k - A_kx_{k-1}^{\star} \end{bmatrix}, \quad
\lambda_{k} = \begin{bmatrix} \lambda_{k1} \\ \vdots \\ \lambda_{kk} \end{bmatrix} \in \mathbb{R}^{d_k}, \quad
d_k = \sum_{i=1}^{k}m_i.
\end{align*}
%
The recursion in initialized with $x_0^{\star} = 0$. After performing the $P$ steps from above, one
obtains a solution pair $(x^{\star}, \Lambda^{\star})$ with
%
\begin{align}
x^{\star} = x_P^{\star}, \quad
\Lambda^{\star} =
\begin{bmatrix}
  \lambda_{11}^{\star} & \lambda_{21}^{\star} & \dots  &\lambda_{P1}^{\star} \\
                       & \lambda_{22}^{\star} & \dots  &\lambda_{P2}^{\star} \\
                       &                      & \ddots & \vdots \\
                       &                      &        &\lambda_{PP}^{\star} \\
\end{bmatrix}.
\end{align}

\newpage

\section{Derivation (of the dual approach)}

\subsection{First objective}

\begin{itemize}

\item The primal problem
  %
  \begin{align}
    \begin{split}
      \minimize{x, \, r_1} & f_1(r_1) = \frac{1}{2}\norm{r_1}^2 + \frac{\mu_1^2}{2}\norm{x}^2\\
      \st & r_1 = A_1x - y_1.
    \end{split}
  \end{align}

\item The Lagrangian
  %
  \[
  L_1(x,r_1,\lambda_{11}) = \frac{1}{2}r_1^Tr_1 + \frac{\mu_1^2}{2} x^Tx + \lambda_{11}^T(A_1x - y_1 - r_1).
  \]

\item The dual function
%
\[
g_1(\lambda_{11}) = \inf_{x,r_1} \underbrace{\frac{1}{2}r_1^Tr_1 - \lambda_{11}^Tr_1} +
\underbrace{\frac{\mu_1^2}{2}x^Tx + \lambda_{11}^TA_1x} - \lambda_{11}^Ty_1.
\]

Minimizing w.r.t. $x$ and then $r_1$ gives
%
\begin{align} \label{eq.x_r}
x^{\star} = -\frac{1}{\mu_1^2}A_1^T\lambda_{11}, \quad r_1^{\star} = \lambda_{11}.
\end{align}
%
Substituting back leads to
%
\begin{align}
\frac{1}{2}r_1^Tr_1 - \lambda_{11}^Tr_1 &= -\frac{1}{2}\lambda_{11}^T\lambda_{11} \\
\frac{\mu_1^2}{2} x^Tx + \lambda_{11}^TA_1x &= -\frac{1}{2\mu_1^2} \lambda_{11}^T A_1A_1^T \lambda_{11}.
\end{align}
%
Hence,
%
\begin{align}
g_1(\lambda_{11}) = -\frac{1}{2\mu_1^2}\lambda_{11}^T\left(\mu_1^2I+A_1A_1^T\right)\lambda_{11} - \lambda_{11}^Ty_1.
\end{align}
%
Note that due to the regularization, the dual function cannot take infinite values and hence there
is no need to have constraints.

\item The dual problem
%
\begin{align}
  \minimize{\lambda_{11}} -g_1(\lambda_{11}) = \frac{1}{2\mu_1^2}\lambda_{11}^T\left(\mu_1^2 I+A_1A_1^T\right)\lambda_{11} + \lambda_{11}^Ty_1,
\end{align}
%
or equivalently
%
\begin{align}
  \minimize{\lambda_{11}} \lambda_{11}^T\left(\mu_1^2 I+A_1A_1^T\right)\lambda_{11} + 2\mu_1^2\lambda_{11}^Ty_1.
\end{align}
%
This can be formulated as the following least-squares problem
%
\begin{align}
\minimize{\lambda_{11}} & \norm{\begin{bmatrix}A_1^T \\ \mu_1I \end{bmatrix}\lambda_{11} - \begin{bmatrix} 0 \\ \mu_1y_1 \end{bmatrix}}^2.
\end{align}

\end{itemize}

\newpage

\subsection{Second objective}

\begin{itemize}

\item The primal problem
  %
  \begin{align}
    \begin{split}
      \minimize{x, \, r_2} & f_2(r_2) = \frac{1}{2}\norm{r_2}^2 + \frac{\mu_2^2}{2}\norm{x}^2\\
      \st & r_2 = A_2x - y_2 \\
      & r_1^{\star} = A_1x - y_1.
    \end{split}
  \end{align}

\item The Lagrangian
  %
  \[
  L_1(x,r_2,\lambda_{21},\lambda_{22}) = \frac{1}{2}r_2^Tr_2 + \frac{\mu_2^2}{2} x^Tx + \lambda_{21}^T(A_1x - y_1 - r_1^{\star}) + \lambda_{22}^T(A_2x - y_2 - r_2).
  \]

\item The dual function
%
\[
g_1(\lambda_{21}, \lambda_{22}) = \inf_{x,r_2} \underbrace{\frac{1}{2}r_2^Tr_2 - \lambda_{22}^Tr_2} + \underbrace{\frac{\mu_2^2}{2}x^Tx + \lambda_{21}^TA_1x + \lambda_{22}^TA_2x} -
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T \begin{bmatrix} y_1+r_1^{\star} \\ y_2 \end{bmatrix}.
\]

Minimizing w.r.t. $x$ and then $r_2$ gives
%
\begin{align} \label{eq.x_r}
x^{\star} = -\frac{1}{\mu_2^2}\sum_{i=1}^{2}A_i^T\lambda_{2i}, \quad r_2^{\star} = \lambda_{22}.
\end{align}
%
Substituting back leads to
%
\begin{align}
\frac{1}{2}r_2^Tr_2 - \lambda_{22}^Tr_2 &= -\frac{1}{2}\lambda_{22}^T\lambda_{22} \\
\frac{\mu_2^2}{2} x^Tx + \lambda_{21}^TA_1x + \lambda_{22}^TA_2x
&= -\frac{1}{2\mu_2^2} \begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T
\begin{bmatrix} A_1A_1^T & A_1A_2^T \\ A_2A_1^T & A_2A_2^T \end{bmatrix}
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}.
\end{align}
%
Hence,
%
\begin{align}
g_2(\lambda_{21},\lambda_{22}) =
-\frac{1}{2\mu_2^2} \begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T
\begin{bmatrix} A_1A_1^T & A_1A_2^T \\ A_2A_1^T & \mu_2^2 I+A_2A_2^T \end{bmatrix}
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix} -
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T \begin{bmatrix} y_1+r_1^{\star} \\ y_2 \end{bmatrix}.
\end{align}

\item The dual problem
%
\begin{align}
\minimize{\lambda_{2}} &
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T
\begin{bmatrix} A_1A_1^T & A_1A_2^T \\ A_2A_1^T & \mu_2^2 I+A_2A_2^T \end{bmatrix}
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix} +
2\mu_2^2
\begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix}^T \begin{bmatrix} y_1+r_1^{\star} \\ y_2 \end{bmatrix}.
\end{align}
%
This can be formulated as the following least-squares problem
%
\begin{align}
  \minimize{\lambda_{21},\ \lambda_{22}} & \norm{\begin{bmatrix}A_1^T & A_2^T \\ 0 & \mu_2I \end{bmatrix}
    \begin{bmatrix} \lambda_{21} \\ \lambda_{22} \end{bmatrix} -
    \mu_2\begin{bmatrix} -\mu_2 x_1^{\star} \\ -\left(y_2 - A_2x_1^{\star}\right) \end{bmatrix}}^2.
\end{align}

\end{itemize}

\end{document}

% ---------------------------------------------------------------------------
